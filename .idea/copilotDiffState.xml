<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/combined_analyzer.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/combined_analyzer.py" />
              <option name="originalContent" value="from cost_constants import *&#10;from utils import analyze_chunks_average&#10;from cost_calculations import calculate_phase1_costs, calculate_phase2_costs, calculate_phase3_costs, calculate_cost&#10;&#10;import os&#10;&#10;def analyze_project_latex(project_root: str, entry_file: str = None, project_name: str = None) -&gt; str:&#10;    &quot;&quot;&quot;Generate LaTeX table format for cost breakdown&quot;&quot;&quot;&#10;    if project_name is None:&#10;        project_name = os.path.basename(project_root).upper()&#10;&#10;    if entry_file is None:&#10;        entry_file = os.path.join(project_root, &quot;__init__.py&quot;)&#10;        if not os.path.isfile(entry_file):&#10;            python_files = []&#10;            for root, _, files in os.walk(project_root):&#10;                for file in files:&#10;                    if file.endswith(&quot;.py&quot;):&#10;                        python_files.append(os.path.join(root, file))&#10;            entry_file = python_files[0] if python_files else &quot;&quot;&#10;&#10;    chunk_analysis = analyze_chunks_average(project_root)&#10;    avg_chunk_tokens = chunk_analysis['avg_tokens_per_chunk']&#10;    total_tokens = chunk_analysis['total_tokens']&#10;    files_processed = chunk_analysis['files_processed']&#10;    avg_tokens_per_file = total_tokens / files_processed if files_processed &gt; 0 else 0&#10;&#10;    phase1_costs = calculate_phase1_costs(chunk_analysis)&#10;    phase2_costs = calculate_phase2_costs(avg_chunk_tokens=avg_chunk_tokens)&#10;&#10;    # Baseline strategy: input tokens = avg_chunk_tokens * 3&#10;    baseline_input_tokens = int(avg_chunk_tokens * 3)&#10;    baseline_output_tokens = int(avg_chunk_tokens)&#10;    baseline_input_cost = calculate_cost(baseline_input_tokens, CLAUDE_INPUT_COST_PER_KTOK)&#10;    baseline_output_cost = calculate_cost(baseline_output_tokens, CLAUDE_OUTPUT_COST_PER_KTOK)&#10;    baseline_total = baseline_input_cost + baseline_output_cost&#10;&#10;    # Chunk-based retrieval (our approach)&#10;    chunk_input_tokens = int(phase2_costs['input_tokens_used'])&#10;    chunk_output_tokens = int(phase2_costs['output_tokens_used'])&#10;    chunk_total = phase2_costs['total_per_prompt']&#10;&#10;    # One-time costs&#10;    embedding_cost = phase1_costs['embedding_cost']&#10;    parsing_cost = phase1_costs['summarization_input_cost'] + phase1_costs['summarization_output_cost']&#10;&#10;    latex_output = f&quot;&quot;&quot;\\begin{{table}}[htbp]&#10;\\caption{{Cost Breakdown for {project_name}}}&#10;\\resizebox{{\\columnwidth}}{{!}}{{%&#10;\\begin{{tabular}}{{|l|r|r|r|}}&#10;\\hline&#10;\\textbf{{Strategy}} &amp; \\textbf{{Input Tokens}} &amp; \\textbf{{Output Tokens}} &amp; \\textbf{{Cost (USD)}} \\\\&#10;\\hline&#10;Baseline &amp; {baseline_input_tokens} &amp; {baseline_output_tokens} &amp; {baseline_total:.6f} \\\\&#10;\\textbf{{Chunk-Based Retrieval (Ours)}} &amp; {chunk_input_tokens} &amp; {chunk_output_tokens} &amp; {chunk_total:.6f} \\\\&#10;\\hline&#10;\\multicolumn{{4}}{{|l|}}{{\\textbf{{One-Time Costs (Ours only)}}}} \\\\&#10;\\hline&#10;Embedding Generation &amp; -- &amp; -- &amp; {embedding_cost:.4f} \\\\&#10;Parsing Cost &amp; -- &amp; -- &amp; {parsing_cost:.4f} \\\\&#10;\\hline&#10;\\end{{tabular}}&#10;}}&#10;\\label{{tab:cost_{project_name}}}&#10;\\end{{table}}&quot;&quot;&quot;&#10;&#10;    return latex_output&#10;&#10;def analyze_project_comprehensive(project_root: str, entry_file: str = None) -&gt; str:&#10;    if entry_file is None:&#10;        entry_file = os.path.join(project_root, &quot;__init__.py&quot;)&#10;        if not os.path.isfile(entry_file):&#10;            python_files = []&#10;            for root, _, files in os.walk(project_root):&#10;                for file in files:&#10;                    if file.endswith(&quot;.py&quot;):&#10;                        python_files.append(os.path.join(root, file))&#10;            entry_file = python_files[0] if python_files else &quot;&quot;&#10;&#10;    chunk_analysis = analyze_chunks_average(project_root)&#10;    avg_chunk_tokens = chunk_analysis['avg_tokens_per_chunk']&#10;    total_tokens = chunk_analysis['total_tokens']&#10;    files_processed = chunk_analysis['files_processed']&#10;    avg_tokens_per_file = total_tokens / files_processed if files_processed &gt; 0 else 0&#10;    avg_tokens_per_project = total_tokens&#10;&#10;    phase1_costs = calculate_phase1_costs(chunk_analysis)&#10;    phase2_costs = calculate_phase2_costs(avg_chunk_tokens=avg_chunk_tokens)&#10;    phase3_costs = calculate_phase3_costs(avg_chunk_tokens=avg_chunk_tokens)&#10;    storage = phase1_costs['storage_details']&#10;&#10;    strategy1_input_cost = calculate_cost(avg_tokens_per_file, CLAUDE_INPUT_COST_PER_KTOK)&#10;    strategy1_output_cost = calculate_cost(avg_chunk_tokens, CLAUDE_OUTPUT_COST_PER_KTOK)&#10;    strategy1_total = strategy1_input_cost + strategy1_output_cost&#10;    strategy2_input_cost = calculate_cost(avg_tokens_per_project, CLAUDE_INPUT_COST_PER_KTOK)&#10;    strategy2_output_cost = calculate_cost(avg_chunk_tokens, CLAUDE_OUTPUT_COST_PER_KTOK)&#10;    strategy2_total = strategy2_input_cost + strategy2_output_cost&#10;&#10;    # Strategy 3: Input = avg file tokens + 2x avg chunk tokens, Output = 1x avg chunk tokens&#10;    strategy3_input_tokens = avg_tokens_per_file + 2 * avg_chunk_tokens&#10;    strategy3_input_cost = calculate_cost(strategy3_input_tokens, CLAUDE_INPUT_COST_PER_KTOK)&#10;    strategy3_output_cost = calculate_cost(avg_chunk_tokens, CLAUDE_OUTPUT_COST_PER_KTOK)&#10;    strategy3_total = strategy3_input_cost + strategy3_output_cost&#10;&#10;    markdown_output = f&quot;&quot;&quot;&#10;# Comprehensive Code Analyzer Cost Breakdown&#10;&#10;## Project Statistics&#10;- **Files Processed**: {chunk_analysis['files_processed']:,}&#10;- **Total Chunks**: {chunk_analysis['total_chunks']:,}&#10;- **Total Tokens**: {chunk_analysis['total_tokens']:,}&#10;- **Average Tokens per Chunk**: {avg_chunk_tokens:.1f}&#10;- **Average Tokens per File**: {avg_tokens_per_file:.1f}&#10;&#10;## Storage Requirements&#10;- **Raw Code Storage**: {storage['raw_code_gb']:.3f} GB&#10;- **Summary Storage**: {storage['summary_storage_gb']:.3f} GB&#10;- **Vector Embeddings**: {storage['vector_storage_gb']:.3f} GB&#10;- **Metadata Storage**: {storage['metadata_storage_gb']:.3f} GB&#10;&#10;---&#10;&#10;##  Phase 1: One-Time Indexing Costs&#10;&#10;| Component | Cost | Details |&#10;|-----------|------|---------|&#10;| **LLM Summarization (Input)** | ${phase1_costs['summarization_input_cost']:.4f} | {chunk_analysis['total_chunks']:,} chunks × {avg_chunk_tokens:.0f} tokens |&#10;| **LLM Summarization (Output)** | ${phase1_costs['summarization_output_cost']:.4f} | {chunk_analysis['total_chunks']:,} summaries × 150 tokens |&#10;| **Embedding Generation** | ${phase1_costs['embedding_cost']:.4f} | {chunk_analysis['total_chunks']:,} embeddings |&#10;| ** Total One-Time Cost** | **${phase1_costs['total_one_time']:.4f}** | |&#10;&#10;### Monthly Storage Costs&#10;| Storage Type | Cost/Month | Size |&#10;|--------------|------------|------|&#10;| Object Storage (Code) | ${phase1_costs['object_storage_monthly']:.4f} | {storage['raw_code_gb']:.3f} GB |&#10;| Vector Database | ${phase1_costs['vector_db_storage_monthly']:.4f} | {storage['vector_storage_gb']:.3f} GB |&#10;| Metadata Database | ${phase1_costs['metadata_storage_monthly']:.4f} | {storage['metadata_storage_gb']:.3f} GB |&#10;| ** Total Storage/Month** | **${phase1_costs['total_monthly_storage']:.4f}** | |&#10;&#10;---&#10;&#10;##  Phase 2: Per-Prompt Costs&#10;&#10;| Component | Cost | Details |&#10;|-----------|------|---------|&#10;| **Prompt Parsing (Input)** | ${phase2_costs['prompt_parsing_input_cost']:.6f} | Small LLM for intent extraction |&#10;| **Prompt Parsing (Output)** | ${phase2_costs['prompt_parsing_output_cost']:.6f} | ~50 tokens response |&#10;| **Vector Search** | ${phase2_costs['vector_search_cost']:.6f} | 1 search operation |&#10;| **Prompt Embedding** | ${phase2_costs['prompt_embedding_cost']:.6f} | Embed user query |&#10;| **Code Retrieval** | ${phase2_costs['retrieval_cost']:.6f} | Fetch 3 chunks from storage |&#10;| **Final LLM (Input)** | ${phase2_costs['final_input_cost']:.6f} | {phase2_costs['input_tokens_used']:.0f} tokens (prompt + code) |&#10;| **Final LLM (Output)** | ${phase2_costs['final_output_cost']:.6f} | {phase2_costs['output_tokens_used']} tokens response |&#10;| ** Total Per Prompt** | **${phase2_costs['total_per_prompt']:.6f}** | |&#10;&#10;---&#10;&#10;##  Phase 3: Update Costs (when files change)&#10;&#10;| Component | Cost | Details |&#10;|-----------|------|---------|&#10;| **File Monitoring** | ${phase3_costs['monitoring_monthly_cost']:.6f}/month | Per file change detection |&#10;| **Re-summarization (Input)** | ${phase3_costs['resummary_input_cost']:.6f} | Process {phase3_costs['affected_chunks']} changed chunks |&#10;| **Re-summarization (Output)** | ${phase3_costs['resummary_output_cost']:.6f} | Generate new summaries |&#10;| **Re-embedding** | ${phase3_costs['reembedding_cost']:.6f} | Update vector embeddings |&#10;| **Database Updates** | ${phase3_costs['db_update_cost']:.6f} | Update vector + metadata DBs |&#10;| ** Total Per Update** | **${phase3_costs['total_per_update']:.6f}** | |&#10;&#10;---&#10;&#10;##  Cost Summary&#10;&#10;### One-Time Setup&#10;- **Initial Indexing**: ${phase1_costs['total_one_time']:.4f}&#10;- **Storage (per month)**: ${phase1_costs['total_monthly_storage']:.4f}&#10;&#10;### Ongoing Operations&#10;- **Per Prompt**: ${phase2_costs['total_per_prompt']:.6f}&#10;- **Per File Update**: ${phase3_costs['total_per_update']:.6f}&#10;&#10;### Cost Projections&#10;- **100 prompts/day**: ${phase2_costs['total_per_prompt'] * 100:.4f}/day&#10;- **1000 prompts/month**: ${phase2_costs['total_per_prompt'] * 1000:.4f}/month&#10;- **10 file updates/month**: ${phase3_costs['total_per_update'] * 10:.4f}/month&#10;&#10;**Total Monthly Cost (1000 prompts + 10 updates)**: ${phase1_costs['total_monthly_storage'] + (phase2_costs['total_per_prompt'] * 1000) + (phase3_costs['total_per_update'] * 10):.4f}&#10;&#10;---&#10;&#10;##  Per-Query Cost Strategies&#10;&#10;| Strategy | Input Tokens | Input Cost | Output Tokens | Output Cost | Total Cost |&#10;|----------|-------------|------------|--------------|-------------|------------|&#10;| Strategy 1 (Avg File) | {avg_tokens_per_file:.0f} | ${strategy1_input_cost:.4f} | {avg_chunk_tokens:.0f} | ${strategy1_output_cost:.4f} | ${strategy1_total:.4f} |&#10;| Strategy 2 (Project) | {avg_tokens_per_project:.0f} | ${strategy2_input_cost:.4f} | {avg_chunk_tokens:.0f} | ${strategy2_output_cost:.4f} | ${strategy2_total:.4f} |&#10;| Strategy 3 (File + 2 Chunks) | {strategy3_input_tokens:.0f} | ${strategy3_input_cost:.4f} | {avg_chunk_tokens:.0f} | ${strategy3_output_cost:.4f} | ${strategy3_total:.4f} |&#10;&quot;&quot;&quot;&#10;    return markdown_output&#10;&#10;&#10;def analyze_project(project_root: str, entry_file: str = None, format_type: str = &quot;markdown&quot;) -&gt; str:&#10;    &quot;&quot;&quot;Analyze project and return results in specified format&quot;&quot;&quot;&#10;    if format_type == &quot;latex&quot;:&#10;        return analyze_project_latex(project_root, entry_file)&#10;    else:&#10;        return analyze_project_comprehensive(project_root, entry_file)&#10;&#10;&#10;def main():&#10;    project_root = &quot;core/homeassistant&quot;&#10;    entry_file = os.path.join(project_root, &quot;__init__.py&quot;)&#10;    if not os.path.exists(project_root):&#10;        print(f&quot;Project root '{project_root}' not found. Using current directory.&quot;)&#10;        project_root = &quot;.&quot;&#10;        entry_file = &quot;combined_analyzer.py&quot;&#10;    if not os.path.isfile(entry_file):&#10;        entry_file = None&#10;&#10;    # Generate both formats&#10;    print(&quot;=== MARKDOWN FORMAT ===&quot;)&#10;    print(analyze_project(project_root, entry_file, &quot;markdown&quot;))&#10;    print(&quot;\n=== LATEX FORMAT ===&quot;)&#10;    print(analyze_project(project_root, entry_file, &quot;latex&quot;))&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    main()&#10;" />
              <option name="updatedContent" value="from cost_constants import *&#10;from utils import analyze_chunks_average&#10;from cost_calculations import calculate_phase1_costs, calculate_phase2_costs, calculate_phase3_costs, calculate_cost&#10;&#10;import os&#10;&#10;def analyze_project_latex(project_root: str, entry_file: str = None, project_name: str = None) -&gt; str:&#10;    &quot;&quot;&quot;Generate LaTeX table format for cost breakdown&quot;&quot;&quot;&#10;    if project_name is None:&#10;        project_name = os.path.basename(project_root).upper()&#10;&#10;    if entry_file is None:&#10;        entry_file = os.path.join(project_root, &quot;__init__.py&quot;)&#10;        if not os.path.isfile(entry_file):&#10;            python_files = []&#10;            for root, _, files in os.walk(project_root):&#10;                for file in files:&#10;                    if file.endswith(&quot;.py&quot;):&#10;                        python_files.append(os.path.join(root, file))&#10;            entry_file = python_files[0] if python_files else &quot;&quot;&#10;&#10;    chunk_analysis = analyze_chunks_average(project_root)&#10;    avg_chunk_tokens = chunk_analysis['avg_tokens_per_chunk']&#10;    total_tokens = chunk_analysis['total_tokens']&#10;    files_processed = chunk_analysis['files_processed']&#10;    avg_tokens_per_file = total_tokens / files_processed if files_processed &gt; 0 else 0&#10;&#10;    phase1_costs = calculate_phase1_costs(chunk_analysis)&#10;    phase2_costs = calculate_phase2_costs(avg_chunk_tokens=avg_chunk_tokens)&#10;&#10;    # Baseline strategy: input tokens = avg_chunk_tokens * 3&#10;    baseline_input_tokens = int(avg_chunk_tokens * 3)&#10;    baseline_output_tokens = int(avg_chunk_tokens)&#10;    baseline_input_cost = calculate_cost(baseline_input_tokens, CLAUDE_INPUT_COST_PER_KTOK)&#10;    baseline_output_cost = calculate_cost(baseline_output_tokens, CLAUDE_OUTPUT_COST_PER_KTOK)&#10;    baseline_total = baseline_input_cost + baseline_output_cost&#10;&#10;    # Chunk-based retrieval (our approach)&#10;    chunk_input_tokens = int(phase2_costs['input_tokens_used'])&#10;    chunk_output_tokens = int(phase2_costs['output_tokens_used'])&#10;    chunk_total = phase2_costs['total_per_prompt']&#10;&#10;    # One-time costs&#10;    embedding_cost = phase1_costs['embedding_cost']&#10;    parsing_cost = phase1_costs['summarization_input_cost'] + phase1_costs['summarization_output_cost']&#10;&#10;    latex_output = f&quot;&quot;&quot;\\begin{{table}}[htbp]&#10;\\caption{{Cost Breakdown for {project_name}}}&#10;\\resizebox{{\\columnwidth}}{{!}}{{%&#10;\\begin{{tabular}}{{|l|r|r|r|}}&#10;\\hline&#10;\\textbf{{Strategy}} &amp; \\textbf{{Input Tokens}} &amp; \\textbf{{Output Tokens}} &amp; \\textbf{{Cost (USD)}} \\\\&#10;\\hline&#10;Baseline &amp; {baseline_input_tokens} &amp; {baseline_output_tokens} &amp; ${{baseline_total:.6f}} \\\\&#10;\\textbf{{Chunk-Based Retrieval (Ours)}} &amp; {chunk_input_tokens} &amp; {chunk_output_tokens} &amp; ${{chunk_total:.6f}} \\\\&#10;\\hline&#10;\\multicolumn{{4}}{{|l|}}{{\\textbf{{One-Time Costs (Ours only)}}}} \\\\&#10;\\hline&#10;Embedding Generation &amp; -- &amp; -- &amp; ${{embedding_cost:.4f}} \\\\&#10;Parsing Cost &amp; -- &amp; -- &amp; ${{parsing_cost:.4f}} \\\\&#10;\\hline&#10;\\end{{tabular}}&#10;}}&#10;\\label{{tab:cost_{project_name}}}&#10;\\end{{table}}&quot;&quot;&quot;&#10;&#10;    return latex_output&#10;&#10;def analyze_project_comprehensive(project_root: str, entry_file: str = None) -&gt; str:&#10;    if entry_file is None:&#10;        entry_file = os.path.join(project_root, &quot;__init__.py&quot;)&#10;        if not os.path.isfile(entry_file):&#10;            python_files = []&#10;            for root, _, files in os.walk(project_root):&#10;                for file in files:&#10;                    if file.endswith(&quot;.py&quot;):&#10;                        python_files.append(os.path.join(root, file))&#10;            entry_file = python_files[0] if python_files else &quot;&quot;&#10;&#10;    chunk_analysis = analyze_chunks_average(project_root)&#10;    avg_chunk_tokens = chunk_analysis['avg_tokens_per_chunk']&#10;    total_tokens = chunk_analysis['total_tokens']&#10;    files_processed = chunk_analysis['files_processed']&#10;    avg_tokens_per_file = total_tokens / files_processed if files_processed &gt; 0 else 0&#10;    avg_tokens_per_project = total_tokens&#10;&#10;    phase1_costs = calculate_phase1_costs(chunk_analysis)&#10;    phase2_costs = calculate_phase2_costs(avg_chunk_tokens=avg_chunk_tokens)&#10;    phase3_costs = calculate_phase3_costs(avg_chunk_tokens=avg_chunk_tokens)&#10;    storage = phase1_costs['storage_details']&#10;&#10;    strategy1_input_cost = calculate_cost(avg_tokens_per_file, CLAUDE_INPUT_COST_PER_KTOK)&#10;    strategy1_output_cost = calculate_cost(avg_chunk_tokens, CLAUDE_OUTPUT_COST_PER_KTOK)&#10;    strategy1_total = strategy1_input_cost + strategy1_output_cost&#10;    strategy2_input_cost = calculate_cost(avg_tokens_per_project, CLAUDE_INPUT_COST_PER_KTOK)&#10;    strategy2_output_cost = calculate_cost(avg_chunk_tokens, CLAUDE_OUTPUT_COST_PER_KTOK)&#10;    strategy2_total = strategy2_input_cost + strategy2_output_cost&#10;&#10;    # Strategy 3: Input = avg file tokens + 2x avg chunk tokens, Output = 1x avg chunk tokens&#10;    strategy3_input_tokens = avg_tokens_per_file + 2 * avg_chunk_tokens&#10;    strategy3_input_cost = calculate_cost(strategy3_input_tokens, CLAUDE_INPUT_COST_PER_KTOK)&#10;    strategy3_output_cost = calculate_cost(avg_chunk_tokens, CLAUDE_OUTPUT_COST_PER_KTOK)&#10;    strategy3_total = strategy3_input_cost + strategy3_output_cost&#10;&#10;    markdown_output = f&quot;&quot;&quot;&#10;# Comprehensive Code Analyzer Cost Breakdown&#10;&#10;## Project Statistics&#10;- **Files Processed**: {chunk_analysis['files_processed']:,}&#10;- **Total Chunks**: {chunk_analysis['total_chunks']:,}&#10;- **Total Tokens**: {chunk_analysis['total_tokens']:,}&#10;- **Average Tokens per Chunk**: {avg_chunk_tokens:.1f}&#10;- **Average Tokens per File**: {avg_tokens_per_file:.1f}&#10;&#10;## Storage Requirements&#10;- **Raw Code Storage**: {storage['raw_code_gb']:.3f} GB&#10;- **Summary Storage**: {storage['summary_storage_gb']:.3f} GB&#10;- **Vector Embeddings**: {storage['vector_storage_gb']:.3f} GB&#10;- **Metadata Storage**: {storage['metadata_storage_gb']:.3f} GB&#10;&#10;---&#10;&#10;##  Phase 1: One-Time Indexing Costs&#10;&#10;| Component | Cost | Details |&#10;|-----------|------|---------|&#10;| **LLM Summarization (Input)** | ${phase1_costs['summarization_input_cost']:.4f} | {chunk_analysis['total_chunks']:,} chunks × {avg_chunk_tokens:.0f} tokens |&#10;| **LLM Summarization (Output)** | ${phase1_costs['summarization_output_cost']:.4f} | {chunk_analysis['total_chunks']:,} summaries × 150 tokens |&#10;| **Embedding Generation** | ${phase1_costs['embedding_cost']:.4f} | {chunk_analysis['total_chunks']:,} embeddings |&#10;| ** Total One-Time Cost** | **${phase1_costs['total_one_time']:.4f}** | |&#10;&#10;### Monthly Storage Costs&#10;| Storage Type | Cost/Month | Size |&#10;|--------------|------------|------|&#10;| Object Storage (Code) | ${phase1_costs['object_storage_monthly']:.4f} | {storage['raw_code_gb']:.3f} GB |&#10;| Vector Database | ${phase1_costs['vector_db_storage_monthly']:.4f} | {storage['vector_storage_gb']:.3f} GB |&#10;| Metadata Database | ${phase1_costs['metadata_storage_monthly']:.4f} | {storage['metadata_storage_gb']:.3f} GB |&#10;| ** Total Storage/Month** | **${phase1_costs['total_monthly_storage']:.4f}** | |&#10;&#10;---&#10;&#10;##  Phase 2: Per-Prompt Costs&#10;&#10;| Component | Cost | Details |&#10;|-----------|------|---------|&#10;| **Prompt Parsing (Input)** | ${phase2_costs['prompt_parsing_input_cost']:.6f} | Small LLM for intent extraction |&#10;| **Prompt Parsing (Output)** | ${phase2_costs['prompt_parsing_output_cost']:.6f} | ~50 tokens response |&#10;| **Vector Search** | ${phase2_costs['vector_search_cost']:.6f} | 1 search operation |&#10;| **Prompt Embedding** | ${phase2_costs['prompt_embedding_cost']:.6f} | Embed user query |&#10;| **Code Retrieval** | ${phase2_costs['retrieval_cost']:.6f} | Fetch 3 chunks from storage |&#10;| **Final LLM (Input)** | ${phase2_costs['final_input_cost']:.6f} | {phase2_costs['input_tokens_used']:.0f} tokens (prompt + code) |&#10;| **Final LLM (Output)** | ${phase2_costs['final_output_cost']:.6f} | {phase2_costs['output_tokens_used']} tokens response |&#10;| ** Total Per Prompt** | **${phase2_costs['total_per_prompt']:.6f}** | |&#10;&#10;---&#10;&#10;##  Phase 3: Update Costs (when files change)&#10;&#10;| Component | Cost | Details |&#10;|-----------|------|---------|&#10;| **File Monitoring** | ${phase3_costs['monitoring_monthly_cost']:.6f}/month | Per file change detection |&#10;| **Re-summarization (Input)** | ${phase3_costs['resummary_input_cost']:.6f} | Process {phase3_costs['affected_chunks']} changed chunks |&#10;| **Re-summarization (Output)** | ${phase3_costs['resummary_output_cost']:.6f} | Generate new summaries |&#10;| **Re-embedding** | ${phase3_costs['reembedding_cost']:.6f} | Update vector embeddings |&#10;| **Database Updates** | ${phase3_costs['db_update_cost']:.6f} | Update vector + metadata DBs |&#10;| ** Total Per Update** | **${phase3_costs['total_per_update']:.6f}** | |&#10;&#10;---&#10;&#10;##  Cost Summary&#10;&#10;### One-Time Setup&#10;- **Initial Indexing**: ${phase1_costs['total_one_time']:.4f}&#10;- **Storage (per month)**: ${phase1_costs['total_monthly_storage']:.4f}&#10;&#10;### Ongoing Operations&#10;- **Per Prompt**: ${phase2_costs['total_per_prompt']:.6f}&#10;- **Per File Update**: ${phase3_costs['total_per_update']:.6f}&#10;&#10;### Cost Projections&#10;- **100 prompts/day**: ${phase2_costs['total_per_prompt'] * 100:.4f}/day&#10;- **1000 prompts/month**: ${phase2_costs['total_per_prompt'] * 1000:.4f}/month&#10;- **10 file updates/month**: ${phase3_costs['total_per_update'] * 10:.4f}/month&#10;&#10;**Total Monthly Cost (1000 prompts + 10 updates)**: ${phase1_costs['total_monthly_storage'] + (phase2_costs['total_per_prompt'] * 1000) + (phase3_costs['total_per_update'] * 10):.4f}&#10;&#10;---&#10;&#10;##  Per-Query Cost Strategies&#10;&#10;| Strategy | Input Tokens | Input Cost | Output Tokens | Output Cost | Total Cost |&#10;|----------|-------------|------------|--------------|-------------|------------|&#10;| Strategy 1 (Avg File) | {avg_tokens_per_file:.0f} | ${strategy1_input_cost:.4f} | {avg_chunk_tokens:.0f} | ${strategy1_output_cost:.4f} | ${strategy1_total:.4f} |&#10;| Strategy 2 (Project) | {avg_tokens_per_project:.0f} | ${strategy2_input_cost:.4f} | {avg_chunk_tokens:.0f} | ${strategy2_output_cost:.4f} | ${strategy2_total:.4f} |&#10;| Strategy 3 (File + 2 Chunks) | {strategy3_input_tokens:.0f} | ${strategy3_input_cost:.4f} | {avg_chunk_tokens:.0f} | ${strategy3_output_cost:.4f} | ${strategy3_total:.4f} |&#10;&quot;&quot;&quot;&#10;    return markdown_output&#10;&#10;&#10;def analyze_project(project_root: str, entry_file: str = None, format_type: str = &quot;markdown&quot;) -&gt; str:&#10;    &quot;&quot;&quot;Analyze project and return results in specified format&quot;&quot;&quot;&#10;    if format_type == &quot;latex&quot;:&#10;        return analyze_project_latex(project_root, entry_file)&#10;    else:&#10;        return analyze_project_comprehensive(project_root, entry_file)&#10;&#10;&#10;def main():&#10;    project_root = &quot;core/homeassistant&quot;&#10;    entry_file = os.path.join(project_root, &quot;__init__.py&quot;)&#10;    if not os.path.exists(project_root):&#10;        print(f&quot;Project root '{project_root}' not found. Using current directory.&quot;)&#10;        project_root = &quot;.&quot;&#10;        entry_file = &quot;combined_analyzer.py&quot;&#10;    if not os.path.isfile(entry_file):&#10;        entry_file = None&#10;&#10;    # Generate both formats&#10;    print(&quot;=== MARKDOWN FORMAT ===&quot;)&#10;    print(analyze_project(project_root, entry_file, &quot;markdown&quot;))&#10;    print(&quot;\n=== LATEX FORMAT ===&quot;)&#10;    print(analyze_project(project_root, entry_file, &quot;latex&quot;))&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    main()" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/cost_constants.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/cost_constants.py" />
              <option name="updatedContent" value="# Cost Constants and Configuration&#10;CLAUDE_INPUT_COST_PER_KTOK = 0.003        # Cost per 1000 input tokens for Claude&#10;CLAUDE_OUTPUT_COST_PER_KTOK = 0.015       # Cost per 1000 output tokens for Claude&#10;GPT_INPUT_COST_PER_KTOK = 0.001&#10;GPT_OUTPUT_COST_PER_KTOK = 0.004&#10;EMBEDDING_COST_PER_KTOK = 0.00001         # Cost for generating embeddings&#10;OBJECT_STORAGE_COST_PER_GB_MONTH = 0.023  # AWS S3 or similar&#10;VECTOR_DB_STORAGE_COST_PER_GB_MONTH = 0   # Pinecone or similar vector DB storage&#10;METADATA_DB_STORAGE_COST_PER_GB_MONTH = 0 # SQLite/PostgreSQL hosting&#10;PROMPT_PARSING_LLM_INPUT_COST_PER_KTOK = GPT_INPUT_COST_PER_KTOK   # Smaller LLM for intent extraction&#10;PROMPT_PARSING_LLM_OUTPUT_COST_PER_KTOK = GPT_OUTPUT_COST_PER_KTOK&#10;VECTOR_SEARCH_COST_PER_1K_OPS = 0    # Vector DB search operations&#10;PROMPT_EMBEDDING_COST_PER_KTOK = 0.00001  # Cost to embed user query&#10;OBJECT_RETRIEVAL_COST_PER_REQUEST = 0  # Download cost from storage&#10;FILE_MONITORING_COST_PER_FILE_MONTH = 0  # VCS diff or file system monitoring&#10;UPDATE_EMBEDDING_COST_PER_KTOK = 0     # Re-embedding changed chunks&#10;DB_UPDATE_COST_PER_OPERATION = 0       # Database write operations&#10;VECTOR_DB_HOSTING_MONTHLY_COST = 0&#10;VECTOR_DB_INFERENCE_COST_PER_KTOK = VECTOR_SEARCH_COST_PER_1K_OPS&#10;OBJECT_STORAGE_COST_PER_REQUEST = OBJECT_RETRIEVAL_COST_PER_REQUEST&#10;STORAGE_COST_PER_GB_MONTH = OBJECT_STORAGE_COST_PER_GB_MONTH&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>